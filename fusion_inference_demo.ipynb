{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9fe550f",
   "metadata": {},
   "source": [
    "# Fusion Model Inference for VIP Cup 2025\n",
    "\n",
    "This notebook demonstrates how to use the updated inference code with fusion models that combine RGB and IR data for improved detection and tracking performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e60e198",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import necessary libraries including cv2, numpy, matplotlib, and the custom models for multiscale detection and tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4862bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Import custom models and utilities\n",
    "from inference_detection_tracking import SubmissionGenerator\n",
    "from models import multiscale_model as multiscale\n",
    "from models import track_model\n",
    "from src.utils import visualize_tracking_video, visualize_detection_image\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2441abd",
   "metadata": {},
   "source": [
    "## 2. Initialize Fusion Detection Model\n",
    "\n",
    "Set up the multiscale detection model with fusion capabilities, specifying model path, confidence and IoU thresholds, and device configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595914f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_path = \"checkpoints/fusion.pt\"\n",
    "team_name = \"V-Linsight\"\n",
    "conf_threshold = 0.2\n",
    "iou_threshold = 0.1\n",
    "\n",
    "# Initialize the fusion submission generator\n",
    "fusion_generator = SubmissionGenerator(\n",
    "    model_path=model_path,\n",
    "    modality='FUSION',\n",
    "    team_name=team_name,\n",
    "    conf_threshold=conf_threshold,\n",
    "    iou_threshold=iou_threshold,\n",
    "    is_visualize=True\n",
    ")\n",
    "\n",
    "print(f\"Fusion model initialized successfully!\")\n",
    "print(f\"Model path: {model_path}\")\n",
    "print(f\"Confidence threshold: {conf_threshold}\")\n",
    "print(f\"IoU threshold: {iou_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c6d362",
   "metadata": {},
   "source": [
    "## 3. Load and Preprocess RGB and IR Video Data\n",
    "\n",
    "Load RGB and IR video pairs from the validation dataset, implement file matching logic to pair corresponding RGB and IR videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8216804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "data_folder = \"data/Validation_Videos\"\n",
    "rgb_folder = Path(data_folder) / \"RGB\"\n",
    "ir_folder = Path(data_folder) / \"IR\"\n",
    "\n",
    "print(f\"RGB folder exists: {rgb_folder.exists()}\")\n",
    "print(f\"IR folder exists: {ir_folder.exists()}\")\n",
    "\n",
    "# Find RGB and IR video pairs\n",
    "rgb_videos = list(rgb_folder.glob(\"*.mp4\")) if rgb_folder.exists() else []\n",
    "ir_videos = list(ir_folder.glob(\"*.mp4\")) if ir_folder.exists() else []\n",
    "\n",
    "print(f\"\\nFound {len(rgb_videos)} RGB videos\")\n",
    "print(f\"Found {len(ir_videos)} IR videos\")\n",
    "\n",
    "# Display first few video pairs\n",
    "if rgb_videos:\n",
    "    print(\"\\nFirst few RGB videos:\")\n",
    "    for i, video in enumerate(rgb_videos[:3]):\n",
    "        print(f\"  {i+1}. {video.name}\")\n",
    "        \n",
    "        # Find matching IR video\n",
    "        rgb_name = video.stem\n",
    "        matching_ir = fusion_generator.find_matching_ir_file(str(video), data_folder)\n",
    "        if matching_ir:\n",
    "            print(f\"     -> Matching IR: {Path(matching_ir).name}\")\n",
    "        else:\n",
    "            print(f\"     -> No matching IR found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68360e2",
   "metadata": {},
   "source": [
    "## 4. Configure Model Parameters\n",
    "\n",
    "Set up detection parameters including confidence threshold, IoU threshold, and other model-specific configurations for optimal fusion performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c409541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection parameters\n",
    "detection_params = {\n",
    "    'scales': [1.0],\n",
    "    'crop_ratio': 0.65,\n",
    "    'weights': [\n",
    "        0.75,  # original image\n",
    "        1.0,   # top-left\n",
    "        1.0,   # top-right\n",
    "        1.0,   # bottom-left\n",
    "        1.0,   # bottom-right\n",
    "        2.0    # center\n",
    "    ],\n",
    "    'conf_threshold': conf_threshold,\n",
    "    'iou_threshold': iou_threshold\n",
    "}\n",
    "\n",
    "# Post-processing parameters for tracking\n",
    "tracking_params = {\n",
    "    'min_detection_frames': 4,    # Minimum frames to keep detection\n",
    "    'max_missing_frames': 5       # Maximum consecutive missing frames\n",
    "}\n",
    "\n",
    "print(\"Detection Parameters:\")\n",
    "for key, value in detection_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nTracking Parameters:\")\n",
    "for key, value in tracking_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa0f2ff",
   "metadata": {},
   "source": [
    "## 5. Process Video Pairs for Fusion Detection\n",
    "\n",
    "Implement video processing pipeline that handles both RGB and IR inputs simultaneously, perform fusion detection on video pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee676cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first video pair for demonstration\n",
    "if rgb_videos:\n",
    "    rgb_video_path = str(rgb_videos[0])\n",
    "    ir_video_path = fusion_generator.find_matching_ir_file(rgb_video_path, data_folder)\n",
    "    \n",
    "    print(f\"Processing video pair:\")\n",
    "    print(f\"RGB: {Path(rgb_video_path).name}\")\n",
    "    print(f\"IR:  {Path(ir_video_path).name if ir_video_path else 'Not found'}\")\n",
    "    \n",
    "    # Process single video pair\n",
    "    if ir_video_path:\n",
    "        print(f\"\\nProcessing fusion video...\")\n",
    "        video_results = fusion_generator.process_video(rgb_video_path, ir_video_path)\n",
    "        print(f\"Generated {len(video_results)} detection results\")\n",
    "        \n",
    "        # Display first few results\n",
    "        if video_results:\n",
    "            print(f\"\\nFirst 3 detection results:\")\n",
    "            for i, result in enumerate(video_results[:3]):\n",
    "                print(f\"Result {i+1}:\")\n",
    "                print(f\"  Frame: {result['Frame_name']}\")\n",
    "                print(f\"  Class: {result['class_label']}\")\n",
    "                print(f\"  Track ID: {result['track_id']}\")\n",
    "                print(f\"  Confidence: {result['confidence_detection']:.3f}\")\n",
    "                print(f\"  BBox: ({result['x_min_norm']:.3f}, {result['y_min_norm']:.3f}, {result['x_max_norm']:.3f}, {result['y_max_norm']:.3f})\")\n",
    "    else:\n",
    "        print(\"No IR video found, processing RGB only...\")\n",
    "        video_results = fusion_generator.process_video(rgb_video_path, None)\n",
    "        print(f\"Generated {len(video_results)} detection results\")\n",
    "else:\n",
    "    print(\"No RGB videos found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55788b20",
   "metadata": {},
   "source": [
    "## 6. Implement Tracking with Fusion Model\n",
    "\n",
    "Create tracking pipeline using the fusion detection model, process video sequences and generate tracking results with track IDs and motion analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166703ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze tracking results if available\n",
    "if 'video_results' in locals() and video_results:\n",
    "    # Group results by frame for analysis\n",
    "    frames_dict = {}\n",
    "    for result in video_results:\n",
    "        frame_name = result['Frame_name']\n",
    "        if frame_name not in frames_dict:\n",
    "            frames_dict[frame_name] = []\n",
    "        frames_dict[frame_name].append(result)\n",
    "    \n",
    "    print(f\"Tracking Analysis:\")\n",
    "    print(f\"Total frames processed: {len(frames_dict)}\")\n",
    "    \n",
    "    # Analyze track IDs\n",
    "    track_ids = set()\n",
    "    class_counts = {}\n",
    "    for result in video_results:\n",
    "        track_ids.add(result['track_id'])\n",
    "        class_label = result['class_label']\n",
    "        class_counts[class_label] = class_counts.get(class_label, 0) + 1\n",
    "    \n",
    "    print(f\"Unique track IDs: {sorted(track_ids)}\")\n",
    "    print(f\"Class distribution: {class_counts}\")\n",
    "    \n",
    "    # Analyze confidence scores\n",
    "    confidences = [float(result['confidence_detection']) for result in video_results]\n",
    "    if confidences:\n",
    "        print(f\"Confidence scores:\")\n",
    "        print(f\"  Mean: {np.mean(confidences):.3f}\")\n",
    "        print(f\"  Min:  {np.min(confidences):.3f}\")\n",
    "        print(f\"  Max:  {np.max(confidences):.3f}\")\n",
    "    \n",
    "    # Plot confidence distribution\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(confidences, bins=20, alpha=0.7)\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Detection Confidence Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    frame_counts = [len(frames_dict[frame]) for frame in sorted(frames_dict.keys())]\n",
    "    plt.plot(frame_counts, marker='o', alpha=0.7)\n",
    "    plt.xlabel('Frame Index')\n",
    "    plt.ylabel('Number of Detections')\n",
    "    plt.title('Detections per Frame')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c613434",
   "metadata": {},
   "source": [
    "## 7. Generate Results and Visualizations\n",
    "\n",
    "Create visualizations for both RGB and IR tracking results, generate detection confidence scores and timing metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3bc84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate visualizations (if videos were processed)\n",
    "if 'rgb_video_path' in locals() and rgb_video_path:\n",
    "    print(\"Generating visualization videos...\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    \n",
    "    # Note: Visualizations are already generated if is_visualize=True in the generator\n",
    "    # Check if visualization files exist\n",
    "    rgb_viz_path = f\"videos/visualized_rgb_{os.path.basename(rgb_video_path)}\"\n",
    "    ir_viz_path = f\"videos/visualized_ir_{os.path.basename(ir_video_path)}\" if ir_video_path else None\n",
    "    \n",
    "    if os.path.exists(rgb_viz_path):\n",
    "        print(f\"RGB visualization saved to: {rgb_viz_path}\")\n",
    "    else:\n",
    "        print(\"RGB visualization not found (check if visualize=True)\")\n",
    "    \n",
    "    if ir_viz_path and os.path.exists(ir_viz_path):\n",
    "        print(f\"IR visualization saved to: {ir_viz_path}\")\n",
    "    elif ir_video_path:\n",
    "        print(\"IR visualization not found (check if visualize=True)\")\n",
    "    \n",
    "    # Create performance metrics visualization\n",
    "    if 'video_results' in locals() and video_results:\n",
    "        inference_times = [float(result['inference_time_detection (ms)']) for result in video_results]\n",
    "        \n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.hist(inference_times, bins=15, alpha=0.7, color='blue')\n",
    "        plt.xlabel('Inference Time (ms)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Inference Time Distribution')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        # Track length analysis\n",
    "        track_lengths = {}\n",
    "        for result in video_results:\n",
    "            track_id = result['track_id']\n",
    "            if track_id not in track_lengths:\n",
    "                track_lengths[track_id] = 0\n",
    "            track_lengths[track_id] += 1\n",
    "        \n",
    "        lengths = list(track_lengths.values())\n",
    "        plt.hist(lengths, bins=10, alpha=0.7, color='green')\n",
    "        plt.xlabel('Track Length (frames)')\n",
    "        plt.ylabel('Number of Tracks')\n",
    "        plt.title('Track Length Distribution')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        # Confidence vs time\n",
    "        confidences = [float(result['confidence_detection']) for result in video_results]\n",
    "        plt.plot(confidences, alpha=0.7, color='red')\n",
    "        plt.xlabel('Detection Index')\n",
    "        plt.ylabel('Confidence Score')\n",
    "        plt.title('Confidence Over Time')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print performance summary\n",
    "        print(f\"\\nPerformance Summary:\")\n",
    "        print(f\"Average inference time: {np.mean(inference_times):.2f} ms\")\n",
    "        print(f\"Total detections: {len(video_results)}\")\n",
    "        print(f\"Average track length: {np.mean(lengths):.1f} frames\")\n",
    "        print(f\"Total unique tracks: {len(track_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141efef8",
   "metadata": {},
   "source": [
    "## 8. Export Submission CSV\n",
    "\n",
    "Format results according to submission requirements, export normalized coordinates, confidence scores, and timing information to CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1353a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process full dataset and generate submission CSV\n",
    "print(\"Processing full dataset for submission...\")\n",
    "\n",
    "# Output path for submission\n",
    "output_csv_path = f\"submissions/{team_name}_FUSION_submission.csv\"\n",
    "\n",
    "# Process the entire validation dataset\n",
    "fusion_generator.process_dataset(data_folder, output_csv_path)\n",
    "\n",
    "# Read and analyze the generated CSV\n",
    "if os.path.exists(output_csv_path):\n",
    "    df = pd.read_csv(output_csv_path)\n",
    "    \n",
    "    print(f\"\\nSubmission CSV Analysis:\")\n",
    "    print(f\"Total records: {len(df)}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Analyze FPS columns\n",
    "    if 'FPS (GPU)' in df.columns and 'FPS (CPU)' in df.columns:\n",
    "        gpu_fps_avg = df['FPS (GPU)'].astype(float).mean()\n",
    "        cpu_fps_avg = df['FPS (CPU)'].astype(float).mean()\n",
    "        \n",
    "        print(f\"\\nFPS Analysis:\")\n",
    "        print(f\"Average GPU FPS: {gpu_fps_avg:.2f}\")\n",
    "        print(f\"Average CPU FPS: {cpu_fps_avg:.2f}\")\n",
    "        print(f\"GPU speedup: {gpu_fps_avg/cpu_fps_avg:.2f}x\")\n",
    "        \n",
    "        # Visualize FPS comparison\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.hist(df['FPS (GPU)'].astype(float), bins=20, alpha=0.7, label='GPU', color='green')\n",
    "        plt.hist(df['FPS (CPU)'].astype(float), bins=20, alpha=0.7, label='CPU', color='red')\n",
    "        plt.xlabel('FPS')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('FPS Distribution Comparison')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.boxplot([df['FPS (GPU)'].astype(float), df['FPS (CPU)'].astype(float)], \n",
    "                   labels=['GPU', 'CPU'])\n",
    "        plt.ylabel('FPS')\n",
    "        plt.title('FPS Box Plot Comparison')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        speedup = df['FPS (GPU)'].astype(float) / df['FPS (CPU)'].astype(float)\n",
    "        plt.hist(speedup, bins=20, alpha=0.7, color='blue')\n",
    "        plt.xlabel('Speedup Factor (GPU/CPU)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('GPU Speedup Distribution')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Display sample of the CSV\n",
    "    print(f\"\\nFirst 5 rows of submission CSV:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Class distribution\n",
    "    if 'class_label' in df.columns:\n",
    "        class_dist = df['class_label'].value_counts()\n",
    "        print(f\"\\nClass distribution:\")\n",
    "        print(class_dist)\n",
    "    \n",
    "    print(f\"\\nSubmission CSV saved to: {output_csv_path}\")\n",
    "else:\n",
    "    print(\"Failed to generate submission CSV!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f22999",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated how to use the fusion model for VIP Cup 2025 submission generation with the following key features:\n",
    "\n",
    "### âœ… **Completed Features:**\n",
    "1. **Fusion Model Integration** - Successfully combines RGB and IR data\n",
    "2. **Dual Device Processing** - Runs inference on both GPU and CPU\n",
    "3. **FPS Benchmarking** - Measures and compares performance on both devices\n",
    "4. **Enhanced CSV Output** - Includes new FPS (GPU) and FPS (CPU) columns\n",
    "5. **Automatic File Matching** - Intelligently pairs RGB and IR files\n",
    "6. **Comprehensive Visualization** - Shows tracking results and performance metrics\n",
    "\n",
    "### ðŸš€ **Performance Benefits:**\n",
    "- **GPU Acceleration** - Significantly faster inference for real-time applications\n",
    "- **CPU Fallback** - Ensures compatibility across different hardware\n",
    "- **Benchmarking** - Provides detailed performance comparison data\n",
    "\n",
    "### ðŸ“Š **Output Format:**\n",
    "The generated CSV now includes:\n",
    "- All standard submission fields\n",
    "- **FPS (GPU)** - Frames per second on GPU\n",
    "- **FPS (CPU)** - Frames per second on CPU\n",
    "- Enhanced timing and confidence metrics\n",
    "\n",
    "### ðŸ”§ **Usage:**\n",
    "```bash\n",
    "# Run the fusion inference\n",
    "python inference_detection_tracking.py \\\n",
    "    --data_folder \"data/Validation_Videos\" \\\n",
    "    --model \"checkpoints/fusion.pt\" \\\n",
    "    --team_name \"V-Linsight\" \\\n",
    "    --modality \"FUSION\" \\\n",
    "    --conf 0.2 \\\n",
    "    --iou 0.1 \\\n",
    "    --visualize\n",
    "```\n",
    "\n",
    "This updated implementation provides comprehensive fusion model support with dual-device performance benchmarking for optimal VIP Cup 2025 submissions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
